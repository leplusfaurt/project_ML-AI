{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f5b7831",
   "metadata": {},
   "source": [
    "<h1>\n",
    "üêù Projet ML/IA - 'To bee or not to bee'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c082589",
   "metadata": {},
   "source": [
    "<h3>1. Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9463277",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from skimage.io import imread\n",
    "from skimage.measure import regionprops, label\n",
    "from skimage.filters import sobel\n",
    "from skimage.feature import graycomatrix, graycoprops\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix, silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set(style='whitegrid')\n",
    "plt.style.use('seaborn-v0_8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66565bc",
   "metadata": {},
   "source": [
    "<h3>2. Load data and setup paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce27b92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = 'train/'\n",
    "mask_dir = 'train/masks/'\n",
    "test_image_dir = 'test/'  # For images 251-347\n",
    "test_mask_dir = 'test/masks/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e76001",
   "metadata": {},
   "source": [
    "<h5> Load data and display basic stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d791cc83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification file loaded successfully\n",
      "Number of training samples: 250\n",
      "\n",
      "Bug type distribution:\n",
      "bug type\n",
      "Bee                115\n",
      "Bumblebee          100\n",
      "Butterfly           15\n",
      "Hover fly            9\n",
      "Wasp                 9\n",
      "Dragonfly            1\n",
      "Bee & Bumblebee      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Species distribution:\n",
      "species\n",
      "Bombus hortorum                              71\n",
      "Apis mellifera                               58\n",
      "Bombus pascuorum                             25\n",
      "Anthidium manicatum                          19\n",
      "Megachile centuncularis                      17\n",
      "Eristalis                                     9\n",
      "Vespula germanica                             8\n",
      "Anthidium                                     6\n",
      "Macroglossum stellatarum                      5\n",
      "Andrenidae                                    4\n",
      "Pieris rapae                                  4\n",
      "Aglais urticae                                3\n",
      "Thyreus                                       3\n",
      "Polyommatus icarus                            2\n",
      "Bombus lapidarius                             2\n",
      "Xylocopa                                      2\n",
      "Macropis fulvipes ?                           2\n",
      "Trachusa byssina                              2\n",
      "Bombus hortorum x2                            2\n",
      "Anthidium manicatum (+Apis mellifera)         1\n",
      "Apis mellifera & Megachile centuncularis      1\n",
      "Bombus hortorum & Megachile centuncularis     1\n",
      "Papilio machaon                               1\n",
      "Aeshnidae                                     1\n",
      "Isodontia mexicana                            1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    label_df = pd.read_excel('classif.xlsx')\n",
    "    print(\"Classification file loaded successfully\")\n",
    "    print(f\"Number of training samples: {len(label_df)}\")\n",
    "    print(\"\\nBug type distribution:\")\n",
    "    print(label_df['bug type'].value_counts())\n",
    "    print(\"\\nSpecies distribution:\")\n",
    "    print(label_df['species'].value_counts())\n",
    "except Exception as e:\n",
    "    print(f\"Error loading classification file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f598f9f0",
   "metadata": {},
   "source": [
    "<h3>3. Feature Extraction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0b197f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_comprehensive_features(image_path, mask_path):\n",
    "    \"\"\"\n",
    "    Extract images / mask features.\n",
    "    Project requirements and personnal additions\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load image and mask\n",
    "        image = imread(image_path)\n",
    "        mask = imread(mask_path, as_gray=True) > 0\n",
    "        \n",
    "        # Ensure image is 3D (RGB)\n",
    "        if len(image.shape) == 2:\n",
    "            image = np.stack([image, image, image], axis=2)\n",
    "        \n",
    "        # Get region properties\n",
    "        labeled_mask = label(mask.astype(int))\n",
    "        props = regionprops(labeled_mask)\n",
    "        \n",
    "        features = {}\n",
    "        \n",
    "        # 1. REQUIRED: Area ratio (pixels of bug / pixels of full image)\n",
    "        total_pixels = mask.shape[0] * mask.shape[1]\n",
    "        bug_pixels = np.sum(mask)\n",
    "        features['area_ratio'] = bug_pixels / total_pixels\n",
    "        \n",
    "        # 2. REQUIRED: RGB statistics within bug mask\n",
    "        for i, color in enumerate(['R', 'G', 'B']):\n",
    "            if image.shape[2] > i:\n",
    "                channel = image[:, :, i]\n",
    "                values = channel[mask]\n",
    "                if len(values) > 0:\n",
    "                    features[f'{color}_min'] = np.min(values)\n",
    "                    features[f'{color}_max'] = np.max(values)\n",
    "                    features[f'{color}_mean'] = np.mean(values)\n",
    "                    features[f'{color}_median'] = np.median(values)\n",
    "                    features[f'{color}_std'] = np.std(values)\n",
    "                else:\n",
    "                    # Handle empty mask case\n",
    "                    for stat in ['min', 'max', 'mean', 'median', 'std']:\n",
    "                        features[f'{color}_{stat}'] = 0\n",
    "        \n",
    "        # 3. REQUIRED: Shape and symmetry measures\n",
    "        if props:\n",
    "            prop = props[0]  # Take the largest connected component\n",
    "            \n",
    "            # Basic shape features\n",
    "            features['area'] = prop.area\n",
    "            features['perimeter'] = prop.perimeter\n",
    "            features['eccentricity'] = prop.eccentricity\n",
    "            features['solidity'] = prop.solidity\n",
    "            features['extent'] = prop.extent\n",
    "            features['convex_area'] = prop.convex_area\n",
    "            \n",
    "            # Symmetry measures\n",
    "            features['major_axis_length'] = prop.major_axis_length\n",
    "            features['minor_axis_length'] = prop.minor_axis_length\n",
    "            features['aspect_ratio'] = prop.major_axis_length / max(prop.minor_axis_length, 1e-6)\n",
    "            \n",
    "            # Compactness and roundness\n",
    "            features['compactness'] = (prop.perimeter ** 2) / max(prop.area, 1e-6)\n",
    "            features['roundness'] = (4 * np.pi * prop.area) / max(prop.perimeter ** 2, 1e-6)\n",
    "            \n",
    "            # Orientation\n",
    "            features['orientation'] = prop.orientation\n",
    "            \n",
    "        else:\n",
    "            # Default values if no region found\n",
    "            shape_features = ['area', 'perimeter', 'eccentricity', 'solidity', 'extent', \n",
    "                            'convex_area', 'major_axis_length', 'minor_axis_length', \n",
    "                            'aspect_ratio', 'compactness', 'roundness', 'orientation']\n",
    "            for feat in shape_features:\n",
    "                features[feat] = 0\n",
    "        \n",
    "        # 4. ADDITIONAL FEATURES (at least 2 more)\n",
    "        \n",
    "        # Texture features using Gray-Level Co-occurrence Matrix (GLCM)\n",
    "        if len(image.shape) == 3:\n",
    "            gray_image = np.mean(image, axis=2).astype(np.uint8)\n",
    "        else:\n",
    "            gray_image = image.astype(np.uint8)\n",
    "        \n",
    "        # Apply mask to gray image\n",
    "        masked_gray = gray_image.copy()\n",
    "        masked_gray[~mask] = 0\n",
    "        \n",
    "        # Calculate GLCM properties\n",
    "        try:\n",
    "            # Reduce levels for GLCM computation\n",
    "            gray_levels = 32\n",
    "            gray_reduced = (masked_gray[mask] / 255 * (gray_levels - 1)).astype(np.uint8)\n",
    "            \n",
    "            if len(gray_reduced) > 10:  # Ensure we have enough pixels\n",
    "                # Create a small region for GLCM\n",
    "                bbox = props[0].bbox if props else (0, 0, min(50, mask.shape[0]), min(50, mask.shape[1]))\n",
    "                y1, x1, y2, x2 = bbox\n",
    "                region = gray_image[y1:y2, x1:x2]\n",
    "                region_mask = mask[y1:y2, x1:x2]\n",
    "                \n",
    "                if np.sum(region_mask) > 0:\n",
    "                    region_masked = region.copy()\n",
    "                    region_masked[~region_mask] = 0\n",
    "                    region_reduced = (region_masked / 255 * (gray_levels - 1)).astype(np.uint8)\n",
    "                    \n",
    "                    # Compute GLCM\n",
    "                    glcm = graycomatrix(region_reduced, [1], [0], levels=gray_levels, symmetric=True, normed=True)\n",
    "                    \n",
    "                    # Extract texture properties\n",
    "                    features['contrast'] = graycoprops(glcm, 'contrast')[0, 0]\n",
    "                    features['dissimilarity'] = graycoprops(glcm, 'dissimilarity')[0, 0]\n",
    "                    features['homogeneity'] = graycoprops(glcm, 'homogeneity')[0, 0]\n",
    "                    features['energy'] = graycoprops(glcm, 'energy')[0, 0]\n",
    "                else:\n",
    "                    features['contrast'] = 0\n",
    "                    features['dissimilarity'] = 0\n",
    "                    features['homogeneity'] = 0\n",
    "                    features['energy'] = 0\n",
    "            else:\n",
    "                features['contrast'] = 0\n",
    "                features['dissimilarity'] = 0\n",
    "                features['homogeneity'] = 0\n",
    "                features['energy'] = 0\n",
    "        except:\n",
    "            features['contrast'] = 0\n",
    "            features['dissimilarity'] = 0\n",
    "            features['homogeneity'] = 0\n",
    "            features['energy'] = 0\n",
    "        \n",
    "        # Edge density feature\n",
    "        try:\n",
    "            # Calculate edge density within the mask\n",
    "            edges = sobel(gray_image)\n",
    "            edge_density = np.sum(edges[mask]) / max(np.sum(mask), 1)\n",
    "            features['edge_density'] = edge_density\n",
    "        except:\n",
    "            features['edge_density'] = 0\n",
    "        \n",
    "        # Color distribution features\n",
    "        # HSV color space analysis\n",
    "        try:\n",
    "            from skimage.color import rgb2hsv\n",
    "            hsv_image = rgb2hsv(image)\n",
    "            \n",
    "            for i, channel in enumerate(['H', 'S', 'V']):\n",
    "                channel_values = hsv_image[:, :, i][mask]\n",
    "                if len(channel_values) > 0:\n",
    "                    features[f'{channel}_mean'] = np.mean(channel_values)\n",
    "                    features[f'{channel}_std'] = np.std(channel_values)\n",
    "                else:\n",
    "                    features[f'{channel}_mean'] = 0\n",
    "                    features[f'{channel}_std'] = 0\n",
    "        except:\n",
    "            for channel in ['H', 'S', 'V']:\n",
    "                features[f'{channel}_mean'] = 0\n",
    "                features[f'{channel}_std'] = 0\n",
    "        \n",
    "        return features\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99f1ca0",
   "metadata": {},
   "source": [
    "<h3>4. Extract features from training set</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100a438f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extracting features from training images...\")\n",
    "feature_list = []\n",
    "failed_extractions = []\n",
    "\n",
    "for i, row in label_df.iterrows():\n",
    "    img_id = row['ID']\n",
    "    img_path = os.path.join(image_dir, f\"{img_id}.JPG\")\n",
    "    mask_path = os.path.join(mask_dir, f\"binary_{img_id}.tif\")\n",
    "    \n",
    "    # We make sure to check if image and mask exist for each ID\n",
    "    try:\n",
    "        if not os.path.exists(img_path):\n",
    "            print(f\"[WARNING] Image missing: {img_path}\")\n",
    "            failed_extractions.append(img_id)\n",
    "            continue\n",
    "        if not os.path.exists(mask_path):\n",
    "            print(f\"[WARNING] Mask missing: {mask_path}\")\n",
    "            failed_extractions.append(img_id)\n",
    "            continue\n",
    "        \n",
    "        features = extract_comprehensive_features(img_path, mask_path)\n",
    "        if features is not None:\n",
    "            features['ID'] = img_id\n",
    "            features['bug_type'] = row['bug type']\n",
    "            features['species'] = row['species']\n",
    "            feature_list.append(features)\n",
    "        else:\n",
    "            failed_extractions.append(img_id)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Problem with ID {img_id}: {e}\")\n",
    "        failed_extractions.append(img_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ae4bf1",
   "metadata": {},
   "source": [
    "<h3> Storing extracted features into a pandas dataframe and extracting into csv format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15ed898",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = pd.DataFrame(feature_list)\n",
    "print(f\"\\nFeature extraction completed!\")\n",
    "print(f\"Successfully processed: {len(df_features)} images\")\n",
    "print(f\"Failed extractions: {len(failed_extractions)} images\")\n",
    "print(f\"Feature dimensions: {df_features.shape}\")\n",
    "\n",
    "# Display basic statistics\n",
    "print(\"\\n=== FEATURE SUMMARY ===\")\n",
    "feature_cols = [col for col in df_features.columns if col not in ['ID', 'bug_type', 'species']]\n",
    "print(f\"Total features extracted: {len(feature_cols)}\")\n",
    "print(\"\\nFeature list:\")\n",
    "for i, feat in enumerate(feature_cols, 1):\n",
    "    print(f\"{i:2d}. {feat}\")\n",
    "\n",
    "# Save features to CSV \n",
    "df_features.to_csv('extracted_features.csv', index=False)\n",
    "print(\"\\nFeatures saved to 'extracted_features.csv'\")\n",
    "\n",
    "print(\"\\n=== SAMPLE DATA ===\")\n",
    "print(df_features.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2342c96",
   "metadata": {},
   "source": [
    "<h3> 5. Data Visualisation </h3>\n",
    "<h5> 5.1 Bug type and species distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00baaa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "bug_counts = df_features['bug_type'].value_counts()\n",
    "plt.pie(bug_counts.values, labels=bug_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "plt.title('Distribution of Bug Types')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "species_counts = df_features['species'].value_counts()\n",
    "plt.bar(range(len(species_counts)), species_counts.values)\n",
    "plt.xticks(range(len(species_counts)), species_counts.index, rotation=45, ha='right')\n",
    "plt.title('Distribution of Species')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "# Cross-tabulation\n",
    "ct = pd.crosstab(df_features['bug_type'], df_features['species'])\n",
    "sns.heatmap(ct, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Bug Type vs Species Cross-tabulation')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85249fed",
   "metadata": {},
   "source": [
    "<h5>5.2 PCA Projection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f0a932",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== PCA ANALYSIS ===\")\n",
    "\n",
    "# Prepare data for PCA\n",
    "X = df_features[feature_cols].fillna(0)\n",
    "y_bug_type = df_features['bug_type']\n",
    "y_species = df_features['species']\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "print(f\"PCA Variance explained by first 2 components: {pca.explained_variance_ratio_}\")\n",
    "print(f\"Total variance explained: {sum(pca.explained_variance_ratio_):.3f}\")\n",
    "\n",
    "# Plot PCA results\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=pd.Categorical(y_bug_type).codes, cmap='tab10')\n",
    "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.3f} variance)')\n",
    "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.3f} variance)')\n",
    "plt.title('PCA Projection - Bug Types')\n",
    "plt.colorbar(scatter, label='Bug Type')\n",
    "\n",
    "# Add legend\n",
    "unique_types = y_bug_type.unique()\n",
    "for i, bug_type in enumerate(unique_types):\n",
    "    plt.scatter([], [], c=plt.cm.tab10(i), label=bug_type)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# Feature importance in PCA\n",
    "feature_importance = np.abs(pca.components_).mean(axis=0)\n",
    "top_features_idx = np.argsort(feature_importance)[-10:]\n",
    "top_features = [feature_cols[i] for i in top_features_idx]\n",
    "top_importance = feature_importance[top_features_idx]\n",
    "\n",
    "plt.barh(range(len(top_features)), top_importance)\n",
    "plt.yticks(range(len(top_features)), top_features)\n",
    "plt.xlabel('Feature Importance in PCA')\n",
    "plt.title('Top 10 Features Contributing to PCA')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7628d532",
   "metadata": {},
   "source": [
    "<h5>5.3 Non-linear projections "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e67449",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== NON-LINEAR PROJECTIONS ===\")\n",
    "\n",
    "# t-SNE projection\n",
    "print(\"Computing t-SNE projection...\")\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(X_scaled)-1))\n",
    "X_tsne = tsne.fit_transform(X_scaled)\n",
    "\n",
    "# UMAP projection (if available, otherwise use another method)\n",
    "try:\n",
    "    from umap import UMAP\n",
    "    print(\"Computing UMAP projection...\")\n",
    "    umap_reducer = UMAP(n_components=2, random_state=42)\n",
    "    X_umap = umap_reducer.fit_transform(X_scaled)\n",
    "    has_umap = True\n",
    "except ImportError:\n",
    "    print(\"UMAP not available, using Isomap instead...\")\n",
    "    from sklearn.manifold import Isomap\n",
    "    isomap = Isomap(n_components=2)\n",
    "    X_umap = isomap.fit_transform(X_scaled)\n",
    "    has_umap = False\n",
    "\n",
    "# Plot non-linear projections\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=pd.Categorical(y_bug_type).codes, cmap='tab10')\n",
    "plt.xlabel('t-SNE 1')\n",
    "plt.ylabel('t-SNE 2')\n",
    "plt.title('t-SNE Projection - Bug Types')\n",
    "# Add legend\n",
    "for i, bug_type in enumerate(unique_types):\n",
    "    plt.scatter([], [], c=plt.cm.tab10(i), label=bug_type)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "scatter = plt.scatter(X_umap[:, 0], X_umap[:, 1], c=pd.Categorical(y_bug_type).codes, cmap='tab10')\n",
    "plt.xlabel('UMAP 1' if has_umap else 'Isomap 1')\n",
    "plt.ylabel('UMAP 2' if has_umap else 'Isomap 2')\n",
    "plt.title(f'{\"UMAP\" if has_umap else \"Isomap\"} Projection - Bug Types')\n",
    "# Add legend\n",
    "for i, bug_type in enumerate(unique_types):\n",
    "    plt.scatter([], [], c=plt.cm.tab10(i), label=bug_type)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63a45de",
   "metadata": {},
   "source": [
    "<h3>6. Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01333bd",
   "metadata": {},
   "source": [
    "<h5>Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adab209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "X_ml = df_features[feature_cols].fillna(0)\n",
    "y_ml = df_features['bug_type']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_ml, y_ml, test_size=0.2, random_state=42, stratify=y_ml)\n",
    "\n",
    "# Standardize features\n",
    "scaler_ml = StandardScaler()\n",
    "X_train_scaled = scaler_ml.fit_transform(X_train)\n",
    "X_test_scaled = scaler_ml.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78981a7a",
   "metadata": {},
   "source": [
    "<h5>6.1 Supervised Methods</h5>\n",
    "<h7>Method 1: Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04738a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- SVM Classifier ---\")\n",
    "svm_clf = SVC(random_state=42)\n",
    "svm_clf.fit(X_train_scaled, y_train)\n",
    "svm_pred = svm_clf.predict(X_test_scaled)\n",
    "\n",
    "print(\"SVM Classification Report:\")\n",
    "print(classification_report(y_test, svm_pred))\n",
    "print(\"SVM Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, svm_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a17d68",
   "metadata": {},
   "source": [
    "<h7>Method 2: K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7785084a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- KNN Classifier ---\")\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_clf.fit(X_train_scaled, y_train)\n",
    "knn_pred = knn_clf.predict(X_test_scaled)\n",
    "\n",
    "print(\"KNN Classification Report:\")\n",
    "print(classification_report(y_test, knn_pred))\n",
    "print(\"KNN Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, knn_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d57485",
   "metadata": {},
   "source": [
    "<h5>6.2 Ensemble Learning Method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c745822a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Random Forest Classifier (Ensemble) ---\")\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_clf.fit(X_train, y_train)  # RF can handle unscaled data\n",
    "rf_pred = rf_clf.predict(X_test)\n",
    "\n",
    "print(\"Random Forest Classification Report:\")\n",
    "print(classification_report(y_test, rf_pred))\n",
    "print(\"Random Forest Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, rf_pred))\n",
    "\n",
    "# Feature importance from Random Forest\n",
    "feature_importance_rf = rf_clf.feature_importances_\n",
    "top_features_rf_idx = np.argsort(feature_importance_rf)[-10:]\n",
    "top_features_rf = [feature_cols[i] for i in top_features_rf_idx]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(len(top_features_rf)), feature_importance_rf[top_features_rf_idx])\n",
    "plt.yticks(range(len(top_features_rf)), top_features_rf)\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Top 10 Features - Random Forest')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a68fb6",
   "metadata": {},
   "source": [
    "<h5>6.3 Clustering Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb0be0c",
   "metadata": {},
   "source": [
    "<h7>Method 1: K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7ccdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- K-Means Clustering ---\")\n",
    "n_clusters = len(df_features['bug_type'].unique())\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "kmeans_labels = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "kmeans_silhouette = silhouette_score(X_scaled, kmeans_labels)\n",
    "print(f\"K-Means Silhouette Score: {kmeans_silhouette:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf990ba",
   "metadata": {},
   "source": [
    "<h7>Method 2: DBSCAN Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b5fea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- DBSCAN Clustering ---\")\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "dbscan_labels = dbscan.fit_predict(X_scaled)\n",
    "\n",
    "n_clusters_dbscan = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)\n",
    "n_noise = list(dbscan_labels).count(-1)\n",
    "print(f\"DBSCAN - Estimated clusters: {n_clusters_dbscan}\")\n",
    "print(f\"DBSCAN - Noise points: {n_noise}\")\n",
    "\n",
    "if n_clusters_dbscan > 1:\n",
    "    dbscan_silhouette = silhouette_score(X_scaled, dbscan_labels)\n",
    "    print(f\"DBSCAN Silhouette Score: {dbscan_silhouette:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bb6297",
   "metadata": {},
   "source": [
    "<h7>Method 3: Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459c78b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Hierarchical Clustering ---\")\n",
    "hierarchical = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "hierarchical_labels = hierarchical.fit_predict(X_scaled)\n",
    "\n",
    "hierarchical_silhouette = silhouette_score(X_scaled, hierarchical_labels)\n",
    "print(f\"Hierarchical Silhouette Score: {hierarchical_silhouette:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3f9af6",
   "metadata": {},
   "source": [
    "<h7>Plot clustering results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0281bf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=kmeans_labels, cmap='tab10')\n",
    "plt.title(f'K-Means Clustering\\n(Silhouette: {kmeans_silhouette:.3f})')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=dbscan_labels, cmap='tab10')\n",
    "plt.title(f'DBSCAN Clustering\\n(Clusters: {n_clusters_dbscan}, Noise: {n_noise})')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=hierarchical_labels, cmap='tab10')\n",
    "plt.title(f'Hierarchical Clustering\\n(Silhouette: {hierarchical_silhouette:.3f})')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef46097",
   "metadata": {},
   "source": [
    "<h5>6.4 Hyperparameter Optimization "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20faa332",
   "metadata": {},
   "source": [
    "<h7> Grid Search for SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6d3077",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- SVM with Grid Search ---\")\n",
    "svm_param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 'auto', 0.01, 0.1, 1],\n",
    "    'kernel': ['rbf', 'linear']\n",
    "}\n",
    "\n",
    "svm_grid_search = GridSearchCV(SVC(random_state=42), svm_param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "svm_grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Best SVM parameters: {svm_grid_search.best_params_}\")\n",
    "print(f\"Best SVM score: {svm_grid_search.best_score_:.3f}\")\n",
    "\n",
    "# Test best SVM\n",
    "best_svm = svm_grid_search.best_estimator_\n",
    "best_svm_pred = best_svm.predict(X_test_scaled)\n",
    "print(\"Optimized SVM Classification Report:\")\n",
    "print(classification_report(y_test, best_svm_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d7b359",
   "metadata": {},
   "source": [
    "<h7>Randomized Search for Random Forest  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd47625",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Random Forest with Randomized Search ---\")\n",
    "rf_param_dist = {\n",
    "    'n_estimators': [50, 100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "rf_random_search = RandomizedSearchCV(RandomForestClassifier(random_state=42), rf_param_dist, \n",
    "                                      n_iter=50, cv=5, scoring='accuracy', n_jobs=-1, random_state=42)\n",
    "rf_random_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best RF parameters: {rf_random_search.best_params_}\")\n",
    "print(f\"Best RF score: {rf_random_search.best_score_:.3f}\")\n",
    "\n",
    "# Test best Random Forest\n",
    "best_rf = rf_random_search.best_estimator_\n",
    "best_rf_pred = best_rf.predict(X_test)\n",
    "print(\"Optimized RF Classification Report:\")\n",
    "print(classification_report(y_test, best_rf_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7810a13c",
   "metadata": {},
   "source": [
    "<h3>7. Models comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1313e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== MODEL COMPARISON ===\")\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "models = {\n",
    "    'SVM': (svm_clf, svm_pred),\n",
    "    'KNN': (knn_clf, knn_pred),\n",
    "    'Random Forest': (rf_clf, rf_pred),\n",
    "    'SVM (Optimized)': (best_svm, best_svm_pred),\n",
    "    'RF (Optimized)': (best_rf, best_rf_pred)\n",
    "}\n",
    "\n",
    "print(\"Model Accuracy Comparison:\")\n",
    "print(\"-\" * 40)\n",
    "for name, (model, pred) in models.items():\n",
    "    accuracy = accuracy_score(y_test, pred)\n",
    "    print(f\"{name:20s}: {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006b8ddc",
   "metadata": {},
   "source": [
    "<h3>8. Prepare for test data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd4bcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== PREPARING FOR TEST DATA ===\")\n",
    "\n",
    "# Select best model based on performance\n",
    "best_model_name = max(models.keys(), key=lambda k: accuracy_score(y_test, models[k][1]))\n",
    "best_model = models[best_model_name][0]\n",
    "\n",
    "print(f\"Selected best model: {best_model_name}\")\n",
    "print(f\"Best model accuracy: {accuracy_score(y_test, models[best_model_name][1]):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44356d13",
   "metadata": {},
   "source": [
    "<h5>8.1 Function to process test images (251 - 347)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf401d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_test_images():\n",
    "    \"\"\"Process test images and generate predictions\"\"\"\n",
    "    test_results = []\n",
    "    \n",
    "    # Assuming test images are numbered 251-347\n",
    "    for img_id in range(251, 348):\n",
    "        img_path = os.path.join(test_image_dir, f\"{img_id}.JPG\")\n",
    "        mask_path = os.path.join(test_mask_dir, f\"binary_{img_id}.tif\")\n",
    "        \n",
    "        if os.path.exists(img_path) and os.path.exists(mask_path):\n",
    "            # Extract features\n",
    "            features = extract_comprehensive_features(img_path, mask_path)\n",
    "            if features is not None:\n",
    "                # Remove non-feature columns\n",
    "                feature_vector = [features.get(col, 0) for col in feature_cols]\n",
    "                \n",
    "                # Scale features using the same scaler\n",
    "                if best_model_name in ['SVM', 'SVM (Optimized)', 'KNN']:\n",
    "                    feature_vector_scaled = scaler_ml.transform([feature_vector])\n",
    "                    prediction = best_model.predict(feature_vector_scaled)[0]\n",
    "                else:\n",
    "                    prediction = best_model.predict([feature_vector])[0]\n",
    "                \n",
    "                test_results.append({'ID': img_id, 'bug type': prediction})\n",
    "    \n",
    "    return test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0204305d",
   "metadata": {},
   "source": [
    "<h5>8.2 Save preprocessing objects for use with test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aeed244",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "preprocessing_objects = {\n",
    "    'scaler': scaler_ml,\n",
    "    'feature_columns': feature_cols,\n",
    "    'best_model': best_model,\n",
    "    'best_model_name': best_model_name\n",
    "}\n",
    "\n",
    "with open('preprocessing_objects.pkl', 'wb') as f:\n",
    "    pickle.dump(preprocessing_objects, f)\n",
    "\n",
    "print(\"Preprocessing objects saved for test data processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3b5d51",
   "metadata": {},
   "source": [
    "<h3> Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a184924b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== PROJECT SUMMARY ===\")\n",
    "print(f\"‚úì Feature extraction completed: {len(feature_cols)} features\")\n",
    "print(f\"‚úì Data visualization: Distributions, PCA, t-SNE, {'UMAP' if has_umap else 'Isomap'}\")\n",
    "print(f\"‚úì Supervised methods: SVM, KNN\")\n",
    "print(f\"‚úì Ensemble method: Random Forest\")\n",
    "print(f\"‚úì Clustering methods: K-Means, DBSCAN, Hierarchical\")\n",
    "print(f\"‚úì Hyperparameter optimization: Grid Search (SVM), Randomized Search (RF)\")\n",
    "print(f\"‚úì Best model selected: {best_model_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
